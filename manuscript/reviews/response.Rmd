---
title: "Response to Comments"
output:
  html_document: default
  html_notebook: default
  pdf_document: default
  word_document: default
---

Thanks to the reviewers and editor for the well thought out comments. I believe that they have greatly imporved, not only the manuscript, but the package it describes as well. I hope that my responses are at least satisfactory. Please find below a copy of the reviewers comments along with my responses.


# -----------Reviewer 1-----------


### Comment 1.1
**Bug**: A graph is produced as expected if the code below is cut-and-pasted at the command line .  However, if the code is put into a file (say `foo.R`) and `source("foo.R")` is executed at the command line, no graph is produced.  (But note that if `plot=TRUE` is added to the call to `partial()`, then a plot is produced as expected.)
```{r, eval=FALSE}
library(randomForest)
data(airquality)
ozone.rf <- randomForest(Ozone ~ ., data=airquality, na.action=na.omit)
library(pdp)
pd <- partial(ozone.rf, pred.var="Temp")
plotPartial(pd)
```

### Response 1.1
This is simply a side effect of using `lattice` to produce the plots. If sourcing a script, then the plot needs to be explicitly printed (e.g., `print(plotPartial(pd))` or `print(partial(fit, pred.var = "x", plot = TRUE))`). Additional text has been added to manuscript (see **Note:** on page 3) and the package documentation explaining this behavior. The fact that this worked when calling `partial` with `plot = TRUE` was a bug (which is now fixed).


### Comment 1.2
I couldn't get `partial` to work with `gbm` (version 2.1.1) models (although I didn't try very hard).  Perhaps this is because the `gbm` maintainer changed the `gbm` interface in a non-backcompatible way. The following (where `gbm.mod` is a model built using the gbm package)
```{r, eval=FALSE}
partial(gbm.mod, pred.var="age")
```
gives
```{r, eval=FALSE}
Error in paste("Using", n.trees, "trees...\n") :
argument "n.trees" is missing, with no default
```
I also tried
```{r, eval=FALSE}
partial(gbm.mod, pred.var="age", n.trees=100)
```
but that gives
```{r, eval=FALSE}
Error in .fun(piece, ...) : unused argument (n.trees = 100)
```

### Response 1.2
This has been fixed in `pdp` (version 0.3.0). The user may still have to supply `n.trees` in the call to `partial` (depending on which version of `gbm` they are using). Also, as suggested in another comment, a `/slowtests` directory is underway and will help prevent future issues like this. Thanks for cathing this.


### Comment 1.3
Standard partial matching isn't implemented (e.g., `partial(..., type="r")`).

### Response 1.3
Partial matching is now implemented (as of `pdp` version 0.3.0), but only for the `type` argument. Thanks for the suggestion.


### Comment 1.4
The following code incorrectly gives an error message
```
"Error in names(pd_df) <- c(pred.var, "y") :
  'names' attribute [2] must be the same length as the vector [1]"
```
It seems to work ok if you change the variable names.
```{r, eval=FALSE}
library(pdp)
data <- data.frame(V1 = 1:10, V2 = 1:10)
mod <- lm(V2 ~ V1, data = data)
partial(mod, pred.var = "V1")  # error
```

### Response 1.4
This has been fixed in `pdp` (version 0.3.0). This was simply a side effect of `partial`'s internal use of `plyr::adply` which uses default variable names (e.g., `V1`, `V2`, etc.). This caused the column `V1` in the output data frame to be overwritten, producing the error. 
```{r, eval=TRUE}
library(pdp)
data <- data.frame(V1 = 1:10, V2 = 1:10)
mod <- lm(V2 ~ V1, data = data)
partial(mod, pred.var = "V1")  # no more error
```


### Comment 1.5
I think you should mention in the documentation something about the assumptions you make about accessing the model data. The following (somewhat contrived) example fails with
`"Error in train[[x]] : subscript out of bounds":`
```{r, eval=FALSE}
data(trees)
foo <- function(data)
{
    lm(Volume ~ ., data = data)
}
mod <- foo(trees)
library(pdp)
partial(mod, pred.var = "Girth", plot = TRUE)  # fails
```
and the following silently gives misleading results:

```{r, eval=FALSE}
library(pdp)
data(trees)
mod <- lm(Volume ~ ., data = trees)
partial(mod, pred.var = "Girth", plot = TRUE)  # ok
trees <- trees[1:2, ]
partial(mod, pred.var = "Girth", plot = TRUE)  # wrong xlim
```
I'm not necessarily saying that you need to resolve the above two
issues, but at least mention in the documentation that this kind of
thing can happen, and how to avoid it (basically call partial in the
same environment used to build the model, and don't change any of the
data used to build the model).

### Response 1.5
Agreed! The documentation for `partial` has also been expanded to discuss this. Unfortunately, some functions just don't store a copy of the training data. However, I have expanded the code to work harder---especially for `S4` methods---to inform the user to supply the training data when necessary. 


### Comment 1.6
The following code gives an obscure error message:
```
"Error in seq.default(from = min(train[[x]], na.rm = TRUE),
   to = max(train[[x]],  :
  'from' cannot be NA, NaN or infinite"
```
Some better handholding for the user may be helpful here.
```{r, eval=FALSE}
library(pdp)
data(trees)
mod <- lm(Volume~., data=trees)
partial(mod, pred.var="nosuchvariable", plot=TRUE) # obscure err msg
```

### Response 1.6
This has been fixed in `pdp` (version 0.3.0) by adding an informative error message listing which variables are the cause of the issue.
```{r, eval=FALSE}
library(pdp)
data(trees)
mod <- lm(Volume~., data=trees)
partial(mod, pred.var=c("Girth", "foo", "bar"), plot=TRUE)
```
```
Error in partial.default(mod, pred.var = c("Girth", "foo", "bar"), plot = TRUE) : foo, bar not found in the training data.
```


### Comment 1.7
On page 2 of the paper it says "The columns of the data frame are labeled in the same order as the features supplied to `pred.var`, and the last column is always labeled `y`". What happens if `y` is one of the predictors?

### Response 1.7
The last column is now labeled `yhat`; which is less likely to be a predictor name. When one of the predictors is labeled `"y"` `randomForest::partialPlot` fails while `gbm::plot.gbm` does not. I think regardless of what the default column name is, there is always the possibility of this issue. This has been filed as an issue on the `pdp` GitHib page: https://github.com/bgreenwell/pdp/issues/22.


### Comment 1.8
Hastie et al. in the book cited in the paper say "Although such a collection [of partial dependence plots] can seldom provide a comprehensive depiction of the approximation, it can often produce helpful clues".  Therefore in in the abstract for the paper, to avoid overselling consider changing "relationship between the outcome and predictors of interest can be easily understood" to "relationship between the outcome and predictors of interest can be more easily understood".

### Response 1.8
Fixed. Thanks for the suggestion.


### Comment 1.9
On page 3 in the PDF of the paper, add a newline to the error message in the paper:
```
Error: The training data could not be extracted from object. Please
supply the raw training data using the ...
```

### Response 1.9
Fixed. Thanks for pointing that out.


### Comment 1.10
Is there any reason the package is called `pdp` but the main function is called `partial`?  Consider giving the function the same name as the package, so e.g., the following works (to help the user get started)
```{r, eval=FALSE}
library(pdp)
?pdp  # fails
```

### Response 1.10
The package was originally called `partial`, but at the time of its original submission this conflicted with a CRAN package of the same name (but with mixed lowercase/uppercase letetrs). So, I decided to change the name of the package to `pdp`; besides, I think `partial` and `plotPartial` work well together as function names. However, I did add a help page describing the package, main functions, etc. So, `?pdp` no longer fails. However, I am not opposed to creating an alias to `partial` called `pdp` in the future.


### Comment 1.11
`partial` should return the same value regardless of whether or not the `plot` argument is used?

### Response 1.11
I dont think so, because I want `partial` to behave like a `lattice` function when using `plot = TRUE` or `plotPartial`; that is, I want it to return a `"trellis"` object. However, I did add the partial dependence data as an attribute to the returned `"trellis"` object (i.e., plot) so that it can easily be extracted if necessary (e.g., if they do not like the plot and don't want to re-run partial). For example,
```{r}
library(pdp)
data(trees)
mod <- lm(Volume ~ ., data = trees)
pd.Girth <- partial(mod, pred.var = "Girth")  # data only
pdp.Girth <- partial(mod, pred.var = "Girth", plot = TRUE)  # plot
identical(pd.Girth, attr(pdp.Girth, "partial.data"))  # should be identical
```
Also, the `lattice` function `trellis.last.object` is now exported when `pdp` is loaded. This will be useful when the user does not store the resulting plot in the call to `partial` but needs to retrieve the outout data to avoid re-running `partial` (e.g., when computation time is a concern). For example,
```{r, fig.width=4, fig.height=4}
library(pdp)
data(trees)
mod <- lm(Volume ~ ., data = trees)
partial(mod, pred.var = "Girth", plot = TRUE)
pdp.Girth <- lattice::trellis.last.object()
pd.Girth <- attr(pdp.Girth, "partial.data")
head(pd.Girth)
```
This is described in the package documentation. Thanks for the idea!


### Comment 1.12
A call to say `par(mfrow = c(2, 2))` before calling `plotPartial` is ignored by `plotPartial`.  This is a pity because it would allow multiple plots to be put on one page like this:
```{r, eval=FALSE}
par(mfrow = c(2, 2))
partial(ozone.rf, pred.var = "Temp", plot = TRUE)
partial(ozone.rf, pred.var = "Wind", plot = TRUE)
```

### Response 1.12
The command `par(mfrow = c(2, 2))` does not work with `"trellis"` objects, such as those produced by `lattice`---which `partial` relies on for producing its plots. Other methods are available. For convenience, `pdp` now imports the `grid.arrange` function from the `gridxtra` package (this has been discussed in the manuscript and incorporated into the examples where appropriate).
```{r, include=FALSE}
library(randomForest)
data(airquality)
ozone.rf <- randomForest(Ozone ~ ., data = airquality, na.action = na.omit)
library(pdp)
```
```{r, eval=TRUE, fig.width=8, fig.height=4}
grid.arrange(
  partial(ozone.rf, pred.var = "Temp", plot = TRUE),
  partial(ozone.rf, pred.var = "Wind", plot = TRUE),
  ncol = 2
)
```
However, since `partial` returns a data frame by default, using `par` would work in the following case
```{r, eval=FALSE}
par(mfrow=c(2,2))
plot(partial(ozone.rf, pred.var = "Temp"), type = "l")
plot(partial(ozone.rf, pred.var = "Wind"), type = "l")
```


### Comment 1.13
It would be nice if the functions automatically plotted all the (important) variables in a grid of plots, without forcing the user to explicitly specify them.

### Response 1.13
I agree, this would be a nice feature, but the problem I see is that not all models naturally emit a "variable importance" score. The `caret` package has a come a long way in this regard. At this point, I think it is simpler to have the user select which variables they would like to see plotted (it forces them to explore the results). I am not opposed to including this feature in the future. I added added a possible solution along with a feature request: https://github.com/bgreenwell/pdp/issues/19. Thanks for the suggestion!


### Comment 1.14
It would be nice if the functions automatically got the data from the model used to build the model in more cases without forcing the user to explicitly specify the data.  Since XGBoost seems now quite popular, perhaps the authors of that package could be persuaded to make the original model data available for functions like partial (currently XGBoost uses a custom object `xgb.DMatrix` which is currently write-only---one can't access the data stored in `xgb.DMatrix`).

### Response 1.14
As mentioned in `Response 1.?`, I have expanded the code to work harder---especially for `S4` methods---to inform the user to supply the training data when necessary. As another example, `kernlab::ksvm` does not store a copy of the training data, at least not as far as I can tell. As time goes on, the coverage will certainly improve, which will make it easier on the user. For example, `partial` now works with `caret` which, by default, stores the taining data in a component called `trainingData`. 

I suspect that the package maintainers for `xgboost` (and similar packages for designed for working with "large" data sets) would be hesitent in storing a copy of the training data with the fitted model object for memory reasons.


### Comment 1.15
Consider adding a `slowtests` directory (below `pdp/inst/` or elsewhere) that has tests for all the models supported by the package (Table 1 in the paper) to easily check back compatibility when packages change.  (For justification for such tests see for example my comments on `gbm` back compatibility above.)  These tests will probably be quite slow, but that won't be an issue because they won't be invoked by CRAN check; the slow tests would be just for the maintainer of the `pdp` package to run manually.

### Response 1.15
This is a good suggestion and is currently under development: https://github.com/bgreenwell/pdp/blob/master/slowtests/slowtests.R. In fact, the `gbm` maintainers have already contacted me about a breaking change caused by the next planned release of their package.


# -----------Reviewer 2-----------


### Comment
The Introduction immediately introduces the PDP. You must first motivate why it is important. In a linear model, they are not important. In a linear model with simple interactions, likewise. They first become important if you have non-parametric machine-learning techniques. Further, they are not the only idea extant when trying to explain a predictor's impact on the response. This paper would benefit if you can give a bit more of a lit review here.

### Response
I agree that PDPs are not as useful in "simple" linear models (e.g., few variables, only two-way interactions, etc.), but certainly there will be cases where PDPs will be extremely useful (e.g., many terms, complex interactions, etc.). In fact, linear models are still common in machine learning/computer science/data science (they just typically contain many terms). I also agree that PDPs are not the only, or sometimes best, approach to understanding the impact of a subset of the predictor's on the response. Brief descriptions and references are given for many alternatives: marginal model plots (`plotmo`), individual conditional expectation plots (`ICEbox`)---which are described as an alternatve to PDPs when strong interactions are suspected, effect plots (`effects`), and a few others available in the `car` package. Unless the editor disagrees, I don't think it is necessary to go into more detail than what is already provided at the end of the introduction. Nonetheless, I added a bit more of a motivational introduction that links to the examples using the Boston housing data. 


## Important Clarifications Required:


### Comment
Top of p2. The `ICEbox` package allows for general construction and visualization of PDP's. However, it is limited as you only get one dimensional representations. The selling point of your package should focus on the flexibility of graphics and the illustration of non-extrapoloation zones.

### Response
`ICEbox` does not actually construct PDPs. Only graphics trying to do the same thing: visualize the relationship between a predictor and the response. I think flexible graphics and non-extrapoloation zones are only half of the selling point here. I think that being able to run `partial` in parallel is huge, and also that this packge works with many types of machine learning models---not just tree-based models. For example, as described in the manuscript, `pdp` can produce single or multi-predictor PDPs for conditional random forests and XGBoost models (among many others), and in parallel. I expanded the last example to describe the interface with `caret`, which I think opens the door for wonderful exploratory data analysis pipe lines (e.g., choose model --> plot importance --> plot relationships, etc.). Most, if any, of these other software packages do not describe or support these important components (e.g., parallel execution), not even ICEbox.


### Comment
All `pdp` figures throughout the paper: Can you make explicit in the caption what code produces the left and right subfigures? I would use the subfigure package in latex and then put a comment in the R code e.g. # Fig 3b.
### Response
Rather than using subfigures, which I think would look messy with all the code, I made quite an attempt to comment which pieces of code produce which figure. Also, in response to another reviewer's comments, the code which produces figures containing multiple plots is better described and demonstrated. See, for example, the section called Multi-predictor PDPs.


### Edits:

### Comment
p1 abstract: strict assumptions imposed by traditional statistical models
### Response
Added some clarification here; in particular, normality, constant variance, and linearity.


### Comment 2.5
p1 pairs should be using curly braces of angle braces, not square braces as this is easily confused with closed intervals.

### Response 2.5
Fixed. Switched to curly braces.


### Comment 2.6
p2 unless the editor disagrees, there is no reason to add "install.packages("pdp")" in the text of the paper as it is assumed a user knows how to install a package.

### Response 2.6
Fixed. Code chunk was removed.


### Comment 2.7
p3 "Error: The training..." should be wrapped.

### Response 2.7
Fixed. The text is now wrapped appropriately. Thanks for catching this!


### Comment 2.8
Fig 6: the sidebar should be labeled "ozone". Also, would it be terrible to label all axes even though it is redundant?

### Response 2.8
The sidebar (i.e., colorkey) has been labeled, but for this example only. As sad as it sounds, labeling the colorkey is not as trivial as we'd expect for `lattice` plots. The code for doing so has been incorporated in the example in question (see pages 8-9). Also, the text describing the first code chunck that produces a display with a colorkey now contains a footnote pointing to the example where the code to produce a label can be found. This is also motivation for why one of the examples showed how to use `ggplot2` for plotting the results from `partial`---`ggplot2` makes it a trivial matter to label the colorkey.


### Comment 2.9
Fig 7: as beautiful as this is, you should label the sidebar as "logit" or "centered logit" instead of "y".

### Response 2.9
Fixed. Color key labeled "Logit".


### Suggestions for the software


### Comment
I understand that you like the feature of having a one-shot function partial to both generate and plot the PDP. I realize that the default is `FALS`E. However, I still think it's a disservice to the user as constructing PDP's is computationally intense and if they get the plot wrong or it's not to their liking, they will have to regenerate the whole thing again. So, it is my opinion that you should take it out and force two steps: generation and illustration.
### Response
While I am not suggesting that the `partial` function needs to mimic the behavior of the PDP fucntions in `randomForest` and `gbm`, I do find it useful to be able to plot the results without forcing the user to store them. However, I agree with your concern which is why `partial` has `plot = FALSE` as the default. Furthermore, I have made it possible to retrieve the raw PDP data even when \code{plot = TRUE}. This is because the "trellis" object returned by `partial` \ `plotPartial` now contains an additional `partial.data` attribute. This is described in the package documentation. For example,
```{r, eval=TRUE}
library(pdp)
data(trees)
mod <- lm(Volume ~ ., data = trees)
partial(mod, pred.var = "Girth", plot = TRUE)
pdp.Girth <- lattice::trellis.last.object()
pd.Girth <- attr(pdp.Girth, "partial.data")
head(pd.Girth)
```


### Comment
The package could be more powerful if you allowed a predict function as a parameter. For instance, you cannot currently use random forests to illustrate classification probabilities.
### Response
I agree, and I have changed the behavior of `...` to refer to additional parameters to `stats::predict` (e.g., the `n.trees` argument in `predict.gbm`.). However, I'm not sure yet about the scale on which the predictions are plotted for classification problems. For now, I am more comfortable leaving it as the usual logit scale defined for PDPs (mostly so that they are defined the same whatever the number of class outcomes). But I am not opposed to making this (simple) change in the future. I have added this as a feature request: https://github.com/bgreenwell/pdp/issues/21.


### Comment 2.12
p7 I believe there are packages allowing for parallelization in Windows. Can you switch the example so this works on all platforms?

### Response 2.12
Good suggestion. I tweaked the example to run on both Windows and Unix-like systems. This is also reflected in the manuscript text.
